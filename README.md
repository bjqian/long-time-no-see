# Large Language Model
The LLM is leading the AI revolution, it would be interesting to understand the underhood of the model itself. 

Intuitively, the math and concepts of LLM should be easy to understand. 

## Tokenization
Tokenization is a process of grouping charactors into tokens. For LLM, this process could compact the input and reduce the context size. It enhances the model's capability of understanding concepts during the training process.

A well adopted tokenization algorithm is BPE. It is originally introduced to compact text. A revised version is used in LLM to tokenize the input to a desired length of lookup table.

It won't take long to implement it. Will come back and add an implementation.
